% Document number 1 of N, part of a concise course in Probability Theory and
% Statistics.
% References:
% [1] A First Course in Probability - 8th Edition - Sheldon Ross

\documentclass[a4paper,twocolumn]{article}

\usepackage{times}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[a4paper,margin=2cm,columnsep=1cm]{geometry}
\usepackage{authblk}
\usepackage{titlesec}
\usepackage[pdftex]{graphicx}
\usepackage{mathtools}
\usepackage{amssymb}


\begin{document}

\graphicspath{{images/}}
\renewcommand{\abstractname}{\normalsize\bfseries\filcenter Abstract}
\titleformat*{\section}{\normalsize\bfseries}
\titleformat*{\subsection}{\small\bfseries}
\renewcommand{\refname}{\normalsize\bfseries\filcenter References}
\renewcommand{\figurename}{\small Figure}
\newcommand{\figureref}[1]{\textit{Figure \ref{fig:#1}}}
\newcommand{\equationref}[1]{\textit{Equation \ref{eq:#1}}}
\newcommand{\Lim}{\lim\limits}
\newcommand{\Bigcup}{\bigcup\limits}
\newcommand{\Bigsum}{\sum\limits}
\newcommand{\notset}[1]{\overline{#1}}


\title{
    \textbf{Probability Theory and Statistics}\\
    \textit{\Large A Very Concise Approach}
}
\author{
    \textbf{Eduardo M. B. de A. Ten√≥rio}\\
    \small\texttt{embatbr@gmail.com}
}
\affil{
    \large{The Men Who Stare at Codes}\\
    \small\texttt{themenwhostareatcodes.wordpress.com}
}
\date{\today}

\maketitle


\section*{Introduction}
\label{sec:intro}

According to the \textit{Encyclopaedia Britannica}, \textbf{probability theory} is ``a branch of mathematics concerned with the analysis of random phenomena" \cite{enc_brit_prob}. This science is used to measure the likeliness that an event will occur. \textbf{Statistics} is defined by \textit{The Oxford Dictionary of Statistical Terms} as ``the study of the collection, analysis, interpretation, presentation and organization of data" \cite{oxford_stat}.

A good example of probability and statistics as a tool is in election polls: researchers use the statistics of the voting population (schooling, region and etc.) to know the probability of victory for each candidate, by taking a sample, generalizing it and calculating the similarity to reality. Machine learning algorithms and insurance policies are also good examples of probability and statistics used to make smart decisions.

This document is a working in progress, where I intend to present the basic topics of Probability Theory and Statistics, so it will change with time. The main reference will be the book \textbf{A First Course in Probability - 8th ed.} \cite{ross_2010}, but probably other materials will be used. If the reader desires to go deeper, further readings are recommended.


\section{Combinatorial Analysis}
\label{sec:comb-anal}   % or "combo-anal" hue hue br

A communication system consisting of a transmitter, four antennas (signal repeaters) and a receiver is organized in a straight line. The system is called \textit{functional} as long as two consecutive antennas are not defective. If we have exactly $m$ of $n$ defective antennas, what is the probability that the resulting system will be functional? For the particular case where we have $n = 4$ and $m = 2$, there are 6 possible configurations, namely,

\begin{center}
0 1 1 0\\
0 1 0 1\\
1 0 1 0\\
0 0 1 1\\
1 0 0 1\\
1 1 0 0
\end{center}

\noindent where 1 means \textit{working} and 0, \textit{defective}. As the resulting system is functional in the first 3 arrangements and not functional in the remaining 3, it is clear that the probability of having a functional system is $\frac{3}{6} = \frac{1}{2}$. So, if we generalize it to undefined $m$ and $n$ values, our probability problem reduces to a counting problem. The mathematical theory of counting is formally known as \textbf{combinatorial analysis}.


\subsection{The Generalized Basic Principle of Counting}
\label{subsec:basic-principle}

If $r$ experiments that are to be performed are such that the first one may result in any of $n_1$ possible outcomes; and if, for each of these $n_1$ possible outcomes, there are $n_2$ possible outcomes of the second experiment; and if, for each of the possible outcomes of the first two experiments, there are $n_3$ possible outcomes of the third experiment; and if \dots , then there is a total of $n_1 n_2 \dotsm n_r = \prod_{i=1}^{r} n_i$ possible outcomes of the $r$ experiments \cite{ross_2010}.

\bigskip

\noindent\textbf{\textit{Example.}} How many different 7-place license plates are possible if the first 3 places are to be occupied by letters and the final 4 by numbers?\\
\noindent\textbf{\textit{Solution.}} By the generalized basic principle of counting, the answer is $26 \cdot 26 \cdot 26 \cdot 10 \cdot 10 \cdot 10 \cdot 10 = 175,760,000$.


\subsection{Permutations}
\label{subsec:permutations}

In mathematics, \textbf{permutation} means ``rearranging the elements of a set in a sequence", and the number of possible sequences is a function of the set's size. Given a set of $n$ elements, we can \textit{permute} it's elements according to the formula:

\begin{equation}
\label{eq:permutation}
P_n = n(n - 1)(n - 2) \dotsm 3 \cdot 2 \cdot 1 = n!
\end{equation}

\noindent As we are arranging the $n$ elements in $n$ positions, the first place can be chosen from $n$ possibilities, the second from $(n - 1)$ and so on, until the last with just one remaining option. We use the $!$ after a arbitrary number $n$ to denote the \textbf{factorial} of $n$.

\bigskip

\noindent\textbf{\textit{Example.}} In a group of 6 men and 4 women, how many possible arrangments can we have if \textbf{\textit{(a)}} put all in the same group, and \textbf{\textit{(b)}} divide by sex?\\
\noindent\textbf{\textit{Solution.}} \textbf{\textit{(a)}} We have 10 people to rearrange into 10 positions, so we just do $P_{10} = 10! = 3,628,800$.\\
\textbf{\textit{(b)}} Using the \textbf{basic principle} we have two different experiments (rearranging men and rearranging women). So, our result is just $P_6 P_4 = 6!4! = 720 \cdot 24 = 17,280$ possibilities.

\bigskip

If our permutation do not preserve beginning and end (called \textbf{circular permutation}), the formula is no longer $n!$ (simple permutation), but $(n - 1)!$. For example, if we permute $abc$ circularly, the results $abc$, $cab$ and $bca$ are equal. The same to $acb$, $bac$ and $cba$. For $n$ elements, as the first $n$ possibilities of the previous ``first position" are the same, we do

\begin{equation}
\label{eq:circ-permutation}
Pc_n = \frac{P_n}{n} = \frac{n!}{n} = (n - 1)!
\end{equation}

\noindent\textbf{Remember:} the order is still important, but now the line does not have a beginning and an end, but a circular organization.


\subsection{$k$-permutations of $n$}
\label{subsec:arrang}

A permutation of $n$ elements in $k$ positions is also called an ``arrangement of $n$, $k$ to $k$" and can be symbolized by $A_{n,k}$. Starting with the simple permutation shown before ($P_n = n!$), if we take the general case and permute the $n$ elements in $k$ positions the first position has $n$ possible values, the second has $(n -1)$ possibilities and so on, until the last position having $(n - k + 1)$ possibilities.

\bigskip

\noindent\textbf{\textit{Example.}} We have 7 athletes in a competition and want to rank them in the three best positions: gold, silver and bronze medals. How many possibilities are there?\\
\noindent\textbf{\textit{Solution.}} The gold medal can be won by all 7 athletes. Ranked the first place, the second has 6 options and the third, 5. This means that $A_{7,3} = 7 \cdot 6 \cdot 5 = 210$.

\bigskip

\noindent So, we can generalize it, think a little and have the simple formula $A_{n,k} = \frac{n!}{(n - k)!}$ for an arrangement.


\subsection{Combinations}
\label{subsec:comb}

If we join the ideas of a circular permutation and arrangement, we develop a new interesting one. In the previous example (the competition), chosen the first three places, we can permute them in $3! = 6$ ways. If we abolish the different medals and just give a ``attaboy" medal to all three (damn liberals!), the six different configurations involving these particular athletes are the same and we can count them as just one. The same to the other 209 configurations. So, now we have $\frac{A_{7,3}}{6} = 35$. This is called \textbf{combination},

\begin{equation}
\label{eq:combination}
C_{n,k} = \binom{n}{k} = \frac{A_{n,k}}{k!} = \frac{n!}{k!(n - k)!}
\end{equation}

\noindent and is read ``combination of $n$, $k$ to $k$". Remember that $k \leq n$ always (but I shouldn't need to say that, aye?).

Now you have the tools to answer the question and go deeper into this subject, if you desire. After see the solution, try to read about \textbf{binomial theorem} and to generalize the combinatorial analysis to \textbf{multinomial coefficients}.


\subsection{Answering The Problem}
\label{subsec:answer-comb-anal}

Let us go back to the antennas problem from the beginning of the section. Given a number of antennas $n = 4$, with $m = 2$ defectives, we name the working ones as $1$ and the defective one as $0$. Let us consider two $0$s and two $1$s, being the set of possibilities $S = \{0_1, 0_2, 1_1, 1_2\}$, just to ilustrative purposes. The configurations are:

\begin{center}
$0_1 0_2 1_1 1_2 = 0 0 1 1$\\
$0_1 1_1 0_2 1_2 = 0 1 0 1$\\
$0_1 1_1 1_2 0_2 = 0 1 1 0$\\
$1_1 0_1 0_2 1_2 = 1 0 0 1$\\
$1_1 0_1 1_2 0_2 = 1 0 1 0$\\
$1_1 1_2 0_1 0_2 = 1 1 0 0$\\
\end{center}

\noindent Take notice that as there is no difference between the $0$s and no difference between the $1$s, the index in each number is irrelevant. So, we are back to the initial configurations and realized we are talking about combinations.

Now, generalizing, we have $n$ antennas, with $m$ defectives. This means we can line up $n - m$ functional antennas, with one or none defective antenna between two functional (and also between the transmitter and the receiver). Than, we have $n - m + 1$ possible positions to fill these spaces and must select $m$ of them. This is $C_{n - m + 1,m} = \binom{n - m + 1}{m}$. Fill it with $n = 4$ and $m = 2$ and we find the same answer.


\section{Axioms of Probability}
\label{sec:axioms}

This section is where we really start to talk about probability theory. The first subsection discusses sample spaces and events, basically \textit{set theory} with a wider vision. So, althought I will explain many basic concepts, it is interesting that you have a basic knowledge about sets.

\subsection{Sample Space and Events}
\label{subsec:sample}

In probability theory we deal with problems of uncertainty, with experiments we cannot guarantee the outcome. However, we know the set of possible outcomes for an experiment, as \textit{head} or \textit{tail} for a coin tossing (not an ``even number" or ``male/female"). This set of possible outcomes for an experiment is called \textbf{sample space} and is denoted $S$. Any subset of $S$ is known as an \textbf{event}. The following examples will clarify these concepts:

\begin{enumerate}
    \item If the experiment consists of flipping two coins, then the sample space consists of the following four pairs:
    \begin{center}
    $S = \{(H,H),(H,T),(T,H),(T,T)\}$
    \end{center}
    where $H = head$ and $T = tail$. Valuable events are $A$ = \{all outcomes are heads\} and $B$ = \{has at least one tail\}, with
    \begin{center}
    $A = \{(H,H)\}$ and $B = \{(H,T),(T,H),(T,T)\}$
    \end{center}

    \item If the experiment consists of tossing two dice, then the sample space consists of the 36 points
    \begin{center}
    $S = \{(i, j): i, j = 1, 2, 3, 4, 5, 6\}$
    \end{center}
    where the outcome $(i, j)$ is said to occur if $i$ appears on the leftmost die and $j$ on the other die. Valuable events are $A = \{i + j = 7\}$ and $B$ = \{all outcomes with $i = 3$\}, with
    \begin{center}
    $A = \{1,6),(2,5),(3,4),(4,3),(5,2),(6,1)\}$ and $B = \{(3,1),(3,2),(3,3),(3,4),(3,5),(3,6)\}$
    \end{center}
\end{enumerate}

Similar to what (I believe) you learned in set theory, events can be \textit{unified}, \textit{intersected}, \textit{complemented} and etc. As this is piece of cake, any doubt can be answered with a quick consultation to a regular book about set theory.


\subsection{Axioms of Probability}
\label{subsec:axioms}

When we say an event $E$ has a probability $P(E) = 30\%$, that doesn't mean we will have exactly 30 occurrences of $E$ for each 100 repetitions of the experiment, but that when the number is really big, the outcomes of $E$ approach this percentage. The leap here is named \textit{relative frequency}, that is the ratio

\begin{equation}
\label{eq:rel-freq}
f(E) = \frac{n(E)}{n}
\end{equation}

\noindent where $n$ is the number of outcomes and $n(E)$ is the number of occurrences of the event $E$. For a small $n$, let us say 10, $n(E)$ hardly is 3. But when $n \to \infty$, $f(E) \to P(E)$, so

\begin{equation}
\label{eq:probability}
P(E) = \Lim_{n \to \infty} \frac{n(E)}{n}
\end{equation}

Now, let us do it properly, showing the \textbf{axioms of probability}. They are few and they are simple. Just a glance and you will think ``of course, this is so obvious".

\begin{description}
    \item[Axiom 1] $0 \leq P(E) \leq 1$
    \item[Axiom 2] $P(S) = 1$
    \item[Axiom 3] $P(\Bigcup_{i=1}^{\infty} E_i) = \Bigsum_{i=1}^{\infty} P(E_i)$, for $E_iE_j = \varnothing$ when $i \neq j$
\end{description}

\noindent Notice that from Axiom 1 an event cannot have a probability greater than the probability of $S$ (Axiom 2). If $P(E) = P(S) \to E = S$. From Axiom 3 we can state that $P(\varnothing) = 0$ (how would you prove that?).

\bigskip

\noindent\textbf{\textit{Example.}} If we roll an honest die (all sides are equally probable) we have $P(\{1\}) = P(\{2\}) = P(\{3\}) = P(\{4\}) = P(\{5\}) = P(\{6\}) = \frac{1}{6}$. From Axiom 3 $P(\{1,2,3\}) = P(\{1\}) + P(\{2\}) + P(\{3\}) = \frac{1}{2}$.

\bigskip

Hopefully, the reader will agree that the axioms are natural and in accordance with our intuitive concept of probability as related to chance and randomness. Furthermore, using these axioms we shall be able to prove that if an experiment is repeated over and over again, then, with probability 1, the proportion of time during which any specific event $E$ occurs will equal $P(E)$ \cite{ross_2010}.

Beyond these easy to understand axioms, there are some propositions we can easily reach. I will leave as an exercise for the reader to find some relations:

\begin{itemize}
    \item Given an event $E$, how would you describe $P(\notset{E})$ in terms of $P(E)$?
    \item If $E \subseteq F$, what is the relation between their probabilities? And for $E \subset F$?
    \item As $P(A \cup B) = P(A) + P(B)$ for $A \cap B = \varnothing$, find the general $P(A \cup B)$. Do the same to $P(\Bigcup_{i=1}^{n}A)$.
\end{itemize}

\noindent You may take a time to prove these three items (specially the third), but after this exercise your knowledges about probability will be better sedimented.


\subsection{Equally Likely Samples}

Suppose we have an closed urn containing 11 balls, 5 red and 6 black. Choosing 3 balls without look, what would be the probability to have 1 black and 2 red?

\bigskip

\noindent\textbf{\textit{Solution.}} Since there is no concern about order, the problem reduces itself to a simple combinatory problem. Using $b$ to black and $r$ to red, the outcomes can be $brr$, $rbr$ or $rrb$. So we can choose 3 balls out of 11, being 1 out of 6 and 2 out of 5. As the choice of each ball is independent, we have

\begin{equation}
\label{eq:balls_problem}
P(E) = \frac{\binom{6}{1}\binom{5}{2}}{\binom{11}{3}} = \frac{6 \cdot 10}{165} = \frac{4}{11}
\end{equation}

Why did this happened? It was not said, but every ball had the same size, friction and etc. The only difference between them was the color, which had no influence in the choice. The balls were \textbf{equally probable}, what means each one of them had the same chance to be chosen. So, if $S = \{s_1, s_2,\dotsm,s_N\}$ and $E_i$ is the event of choose sample $s_i$, then

\begin{equation}
\label{eq:equally_prob}
P(E_i) = \frac{1}{N}, i = 1, 2, \dotsm N
\end{equation}

\noindent To prove it, remember the axioms and how independent these events are.


\begin{thebibliography}{9}
    \bibitem{enc_brit_prob}
         ``Probability theory, Encyclopaedia Britannica".
         Britannica.com.
         Retrieved 2014-10-11

    \bibitem{oxford_stat}
        Dodge, Y. (2006)
        \textit{The Oxford Dictionary of Statistical Terms}, OUP.
        ISBN 0-19-920613-9

    \bibitem{ross_2010}
        Ross, Sheldon M.,
        2010,
        ``A First Course in Probability", 8th ed.,
        Prentice Hall
\end{thebibliography}


\end{document}