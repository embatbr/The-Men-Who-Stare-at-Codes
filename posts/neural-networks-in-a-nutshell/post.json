{
    "title" : "Neural networks in a nutshell",
    "category" : "Learning",
    "tags" : ["neural networks", "neuron", "machine learning", "pattern recognition", "McCulloch-Pitts",
    "Heaviside", "nutshell", "python"],
    "content" : "This is the first post that makes justice to the blog's motto: **show me the code motherfucker**. In this and the next *n* posts with the title \"Neural networks in a nutshell - part *k*\" I will talk about artificial neural networks, showing concepts (theory) and code (practice). The codes will be written in <a href=\"http://python.org/\" target=\"_blank\">Python</a> without any fancy library as <a href=\"http://www.numpy.org/\" target=\"_blank\">NumPy</a>, <a href=\"http://www.scipy.org/\" target=\"_blank\">SciPy</a> or <a href=\"http://pybrain.org/\" target=\"_blank\">PyBrain</a> just because:

1. I don't know how to use any of these.
2. I don't have time to learn them now.
3. The focus is the concept, not the performance.

Also I will learn how to type <a href=\"http://www.latex-project.org/\" target=\"_blank\">$latex \\LaTeX$</a> code and <a href=\"http://en.support.wordpress.com/latex/\" target=\"_blank\">insert it</a>.

## **0 - Overview**

**Artificial neural networks**, **ANNs** or **neural networks**, are computational models inspired by animals' central nervous system (usually the brain) and used to recognize patterns through machine learning. These models are organized as parallel systems composed of simple processing units (*neurons*) disposed in a direct graph structure (*network*) with weighted edges (*synapses*). Although they are based on biological neural networks, ANNs are just simplified versions of those, with only the necessary parts.

An ANN is a **statistical pattern recognition technique**, but it's not unique. ANNs are used when the domain of a problem is not entirely known, so the *net* must learn by examples how to identify the patterns. For example, imagine a financial institution is searching for the best way to determine if a new client is good for a loan. Given a set of caracteristics (*inputs*) as income, credit history, age, occupation and criminal records, and with a corresponding classification (*output*), it is possible to teach the ANN how to identify the majority of good and bad payers. As the examples (the tuples input-output) are presented, the *net* changes it's weights in order to minimize the errors in classifying the inputs. This is an example of an ANN with *supervised* learning, where the correct (or *desired*) output is known *a priori*. Not all ANNs learn with a supervisor, but this is a subject for later posts.

## **1 - The McCullochâ€“Pitts neuron**

As said before, an ANN is a direct graph in which the nodes are neurons and the edges are the synapses. To understand this subject, let's first see how the *McCulloch-Pitts model* works, represented by the diagram in *Fig. 1*. The name was given in recognition of the pioneering work done by Warren McCulloch and Walter Pitts (1943) in modeling neural networks.

[caption id=\"attachment_251\" align=\"aligncenter\" width=\"450\"]<a href=\"http://themenwhostareatcodes.files.wordpress.com/2014/03/neuron-diagram-01.png\"><img class=\"wp-image-251 \" alt=\"neuron-diagram-01\" src=\"http://themenwhostareatcodes.files.wordpress.com/2014/03/neuron-diagram-01.png?w=300\" width=\"450\" height=\"264\" /></a> <strong>Figure 1</strong> Model of a neuron, labeled k.[/caption]

Here we identify three basic elements of this model:

1. A set of *synapses*, each caracterized by a *weight*. For each input (*dendrite signal*) $latex x_j$ in the neuron $latex k$ there is a synaptic weight $latex w_kj$ to multiply it.
2. A *linear combiner* or *adder* for summing the weighted input signals.
3. An *activation function* $latex \\varphi(.)$ for limiting the amplitude of the neuron's output. It is in the interval [0,1].

The model also has an externally applied *bias*, denoted by $latex b_k$ (sometimes called $latex w_0$ with an input $latex x_0 = +1$ as in *Fig. 2*), which effect is to increase or decrease the net input of the activation function.

[caption id=\"attachment_253\" align=\"aligncenter\" width=\"450\"]<a href=\"http://themenwhostareatcodes.files.wordpress.com/2014/03/neuron-diagram-02.png\"><img class=\" wp-image-253  \" alt=\"neuron-diagram-02\" src=\"http://themenwhostareatcodes.files.wordpress.com/2014/03/neuron-diagram-02.png?w=300\" width=\"450\" height=\"273\" /></a> <strong>Figure 2</strong> Another nonlinear model of a neuron, with the bias as an input.[/caption]

All this given, we can describe the neuron *k* by the pair of equations:

1. $latex u_k = \\sum_{j=1}^{m} w_kj x_j$
2. $latex y_k = \\varphi(u_k + b_k)$

where we can say $latex y_k = \\varphi(v_k)$ and $latex v_k = u_k + b_k$. The value of $latex v_k$ is modified depending on the signal of $latex b_k$ as shown in *Fig. 3*.

[caption id=\"attachment_252\" align=\"aligncenter\" width=\"290\"]<a href=\"http://themenwhostareatcodes.files.wordpress.com/2014/03/bias-graphic.png\"><img class=\"wp-image-252  \" alt=\"bias-graphic\" src=\"http://themenwhostareatcodes.files.wordpress.com/2014/03/bias-graphic.png?w=290\" width=\"290\" height=\"300\" /></a> <strong>Figure 3</strong> Affine transformation produced by the presence of a bias.[/caption]

The *activation function* $latex \\varphi(.)$ is the <a href=\"http://en.wikipedia.org/wiki/Heaviside_step_function\" target=\"_blank\">*Heaviside function*</a>, which means the output $latex y_k$ is 1 for any nonnegative value of $latex v_k$ and 0 otherwise, as shown in *Fig. 4*.

[caption id=\"attachment_276\" align=\"aligncenter\" width=\"300\"]<a href=\"http://themenwhostareatcodes.files.wordpress.com/2014/03/heaviside-graphic.png\"><img class=\" wp-image-276 \" alt=\"heaviside-graphic\" src=\"http://themenwhostareatcodes.files.wordpress.com/2014/03/heaviside-graphic.png?w=300\" width=\"300\" height=\"155\" /></a> <strong>Figure 4</strong> The Heaviside function.[/caption]
<p style=\"text-align:center;\"></p>

Now we will see some code. First let's define the *Heaviside function*:

[code language=\"python\"]
def heaviside(x):
    if x >= 0:
        return 1
    else:
        return 0
[/code]

Notice that in this case, for `x == 0` the return is `1`, not `1/2`. Later comes the class defining the neuron (I am no checking if `inputs` and `self.weights` have the same length):

[code language=\"python\"]
class MCP_Neuron(object):

    def __init__(self, weights, bias):
        self.weights = weights
        self.bias = bias

    def fire(self, inputs):
        summed = sum([i*w for (i,w) in zip(inputs, self.weights)])
        return heaviside(summed + self.bias)
[/code]

Notice the *activation function* was not set as an attribute. In a more general model it would (and will) be necessary. Also the implementation is based on the diagram of *Fig. 1*. The other shown in *Fig. 2* would be better for a circuit implementation. And at last we test the model (the negative bias makes the neuron firing more difficult - what usually is our intent):

[code language=\"python\"]
if __name__ == &#039;__main__&#039;:
    neuron_1 = MCP_Neuron([0.2, 0.7, 0.3], -1.5)
    neuron_2 = MCP_Neuron([0.4, 0.6, 0.9], -0.8)
    neuron_3 = MCP_Neuron([0.7, 0.4, -0.9], -0.6)

    inputs = [1, 0, 1]

    print(&#039;Test #1 - inputs on neuron_1:&#039;, neuron_1.fire(inputs))
    print(&#039;Test #1 - inputs on neuron_2:&#039;, neuron_2.fire(inputs))
    print(&#039;Test #1 - inputs on neuron_3:&#039;, neuron_3.fire(inputs))
[/code]

To fit in this page, I created a soft link in the desktop with the line `ln -s ~/dados/workspace/The-Men-Who-Stare-at-Codes/posts/neural-networks-in-a-nutshell/code/ neural-code`.

[code language=\"bash\"]
embat@hal9000:~/desktop/neural-code$ python3.3 mcp.py
Test #1 - inputs on neuron_1: 0
Test #1 - inputs on neuron_2: 1
Test #1 - inputs on neuron_3: 0
embat@hal9000:~/desktop/neural-code$
[/code]

The complete code is in the post directory on <a href=\"https://github.com/embatbr/The-Men-Who-Stare-at-Codes/blob/master/posts/neural-networks-in-a-nutshell/code/mcp.py\" target=\"_blank\">github</a> and that's all for today folks. In the next post we will see the perceptrons.

### References

- <a href=\"http://en.wikipedia.org/wiki/Artificial_neural_network\" target=\"_blank\">Neural Networks - Wikipedia</a>.
- <a href=\"http://www.amazon.com/Neural-Networks-Learning-Machines-Edition/dp/0131471392\" target=\"_blank\">Neural Networks and Learning Machines - 3rd Ed - Simon Haykin - Pearson</a>.

**Live long and prosper**"
}