For now we learnt <a href="http://themenwhostareatcodes.wordpress.com/2014/03/02/neural-networks-in-a-nutshell/" title="McCulloch-Pitts" target="_blank">what an ANN is and it's basic element</a> and the <a href="http://themenwhostareatcodes.wordpress.com/2014/03/07/neural-networks-in-a-nutshell-2/" title="Perceptron" target="_blank">simplest neural net ever</a>. Before start this post properly, let's understand some concepts first.

<!--more-->

The Perceptron converged in our example because the problem was *lineraly separable*, what means we could divide it in two classes using a *hyperplane* (in that case, a *straight line*). That's why I defined the stopping rule as "stop when there's no errors, *bitch*", because someday it happens. If just one example of class `C1` where in the region of class `C2`, we couldn't draw a line dividing both classes. So, if you don't know if the problem is *linearly separable* (what could lead to an endless loop), change the *stopping rule* to abort training after a predefined number of epochs:

[code language="python"]
    def training(self, examples, max_epochs=None):
        epochs = 0

        while True:
            epochs = epochs + 1
            error_count = 0

            for (input_vector, desired_output) in examples:
                actual_output = self.neuron.fire(input_vector)
                error = desired_output - actual_output

                if error != 0:
                    learned = self.lrn_rate*error
                    self.neuron.update(input_vector, learned)
                    error_count = error_count + 1

            if error_count == 0:
                return epochs
            elif max_epochs and (epochs > max_epochs):
                return False
[/code]

But sometimes what doesn't seem to be separable by one line can be by two or more. This way we still can divide the classes in two regions. To understand how to "cheat" this Perceptron's weakness, let's first learn about Multiclass Perceptron.

## 3 - Multiclass Perceptron

Taking our equation from previous post (as shown in *Eq. 7*) let's add another line (*Eq. 8*) crossing the first

- (7) $latex x_2 = -5x_1 + 2$
- (8) $latex x_2 = 3x_1 + 4$

so the plane $latex x_1x_2$ is divided in four different regions:

- C1: above *Eq. 7* and *Eq. 8*
- C2: above *Eq. 7* and below *Eq. 8*
- C3: below *Eq. 7* and above *Eq. 8*
- C4: below *Eq. 7* and *Eq. 8*

Having four regions we need two neurons, one for each line. With two neurons we have two outputs (and four *linearly separable* classes), defined as:

- C1: (output_1, output_2) = (1, 1)
- C2: (output_1, output_2) = (1, -1)
- C3: (output_1, output_2) = (-1, 1)
- C4: (output_1, output_2) = (-1, -1)

Then we need to build a *layer* of perceptrons. Basically what is done is a class containing a list of perceptrons (remember each one is **independent** from the others):

[code language="python"]
class Layer(object):
    """A layer containing two or more perceptrons.
    """
    def __init__(self, input_size, num_perceptrons=2, lrn_rates=[1, 1],
                 activations=[signal, signal]):
        """Not checking if lrn_rates and activations have the length equals to
        num_perceptrons.
        """
        self.perceptrons = [Perceptron(input_size, lrn_rates[i], activations[i])
                            for i in range(num_perceptrons)]

    def fire(self, inputs):
        return [perceptron.fire(inputs) for perceptron in self.perceptrons]

    def training(self, inputs_vector, outputs_vector, max_epochs):
        """outputs_vector is a list containing the same number of elements of
        perceptron. Each element is another list with length equals to inputs_vector's
        length.
        """
        epochs = 0

        for (perceptron, outputs) in zip(self.perceptrons, outputs_vector):
            epochs_per_perceptron = perceptron.training(inputs_vector, outputs,
                                                        max_epochs)
            if not epochs_per_perceptron:
                return epochs_per_perceptron

            epochs = epochs + epochs_per_perceptron

        return epochs
[/code]

It's importante to notice that each *perceptron* receives all inputs, no matter if it's one, two or ten in the layer. Also, as the classes are *linearly separable*, we can train one *perceptron* per time. So, now, our Multiclass Perceptron is composed of two layers: an input layer and an output layer (where the neurons are).

The code to test this new Perceptron of ours (with two neurons) is basically the same:

[code language="python"]
def classify(y, x, a, b):
    if y >= a*x + b:
        return 1 # class C1
    return -1 # class C2

def gen_examples(num_examples, a, b, c, d):
    inputs_vector = []
    outputs_vector = [[], []]

    for _ in range(num_examples):
        x1 = uniform(-10, 10)
        x2 = uniform(-10, 10)

        inputs_vector.append([x1, x2])
        outputs_vector[0].append(classify(x1, x2, a, b))
        outputs_vector[1].append(classify(x1, x2, c, d))

    return (inputs_vector, outputs_vector)

def layer(lrn_rates, num_training, num_test, a, b, c, d):
    layer = Layer(2, lrn_rates=lrn_rates)

    (inputs_vector, outputs_vector) = gen_examples(num_training, a, b, c, d)
    print('#TRAINING (max_epochs = 80)')
    epochs = layer.training(inputs_vector, outputs_vector, 80)
    print(('epochs: %d' % epochs) if epochs else 'training aborted')

    (inputs_vector, outputs_vector) = gen_examples(num_test, a, b, c, d)
    print('#TESTING')
    error_count = load_test(layer, inputs_vector, outputs_vector)
    print('error_count:', error_count)
[/code]

Now, imagine we keep the lines from *Eq. 7* and *Eq. 8*, but again we just have two classes:

- C1: above *Eq. 7* and *Eq. 8*
- C2: otherwise

This means `C2`, `C3` and `C4` are all called just `C2`. There's no way to draw a line separating the new classes `C1` and `C2`, but we could filter the previously shown *layer*'s outputs (two numbers `y1` and `y2`) into just one, using a "*neuron*". This new *neuron*'s filter rule would be something like:

- C1: (`y1`, `y2`) = (1, 1)
- C2: otherwise

Using the output *neuron* with a fixed *bias* in the interval (`1.5`, `-2`] we could define who is `C1` and who is `C2`. But this *filter* would just be used after training the ANN, as it's weights must always be `1`, and throught it's output is impossible to reach `y1` and `y2`. The *activation function* is not **bijective**.

This network described above is a draft of an ANN known as **Multilayer Perceptron**. But before that, as I always do (*bastard*), let me show you the Adaline network.

## 4 - Adaline

Adaline comes from **ADAptive LINear Element** (previously **ADAptive LInear NEuron**), an ANN based on the *McCulloch-Pitts perceptron*, but without the *activation function* (or, more precisely, a *linear activation function*). It's output is given by:

- (9) $latex y = \sum_{j=1}^{n} w_jx_j + b$

As we know, the *perceptron*'s learning rule is given by

- (10) $latex w_j(n + 1) = w_j(n) + \eta(d_j - y_j)x_j$
- (11) $latex b(n + 1) = b(n) + \eta(d_j - y_j)$

where $latex \eta$ is the *learning rate*, $latex d_j$ is the desired output and $latex y_j$ is the actual output. This can be composed into $latex \Delta w_j = \eta(d_j - y_j)x_j$ and *Eq. 10* and *Eq. 11* can be rewritten as:

- (12) $latex w_j(n + 1) = w_j(n) + \Delta w_j$
- (13) $latex b(n + 1) = b(n) + \Delta w_0$ (remember $latex w_0 = b$ and $latex x_0 = +1$)

This *learning rule* is useful to a perceptron, but not to a nonlimited output. So, we need a new *learning rule*.

### Delta rule

## 5 - Multilayer Perceptron

## References

- <a href="http://www.amazon.com/Neural-Networks-Learning-Machines-Edition/dp/0131471392" target="_blank">Neural Networks and Learning Machines – 3rd Ed – Haykin, Simon – Pearson</a>.
- <a href="http://prolland.free.fr/works/ai/docs/neuro-intro.pdf" target="_blank">An introduction to Neural Networks – 8th Ed. – Kröse, Ben; van der Smagt, Patrick – University of Amsterdam</a>.
- <a href="http://en.wikipedia.org/wiki/Delta_rule" target="_blank">Delta rule - Wikipedia</a>.
