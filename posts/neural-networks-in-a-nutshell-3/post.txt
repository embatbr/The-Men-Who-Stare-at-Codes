For now we learnt <a href="http://themenwhostareatcodes.wordpress.com/2014/03/02/neural-networks-in-a-nutshell/" title="McCulloch-Pitts" target="_blank">what an ANN is and it's basic element</a> and the <a href="http://themenwhostareatcodes.wordpress.com/2014/03/07/neural-networks-in-a-nutshell-2/" title="Perceptron" target="_blank">simplest neural net ever</a>. Before start this post properly, let's understand some concepts first.

<!--more-->

The Perceptron converged in our example because the problem was *lineraly separable*, what means we could divide it in two classes using a *hyperplane* (in that case, a *straight line*). That's why I defined the stopping rule as "stop when there's no errors, *bitch*", because someday it happens. If just one example of class `C1` where in the region of class `C2`, we couldn't draw a line dividing both classes. So, if you don't know if the problem is *linearly separable* (what could lead to an endless loop), change the *stopping rule* to abort training after a predefined number of epochs:

[code language="python"]
    def training(self, examples, max_epochs=None):
        epochs = 0

        while True:
            epochs = epochs + 1
            error_count = 0

            for (input_vector, desired_output) in examples:
                actual_output = self.neuron.fire(input_vector)
                error = desired_output - actual_output

                if error != 0:
                    learned = self.lrn_rate*error
                    self.neuron.update(input_vector, learned)
                    error_count = error_count + 1

            if error_count == 0:
                return epochs
            elif max_epochs and (epochs > max_epochs):
                return False
[/code]

But sometimes what doesn't seem to be separable by one line can be by two or more. This way we still can divide the classes in two regions. To understand how to "cheat" this Perceptron's weakness, let's first learn about Multiclass Perceptron.

## 3 - Multiclass Perceptron

Taking our equation from previous post (as shown in *Eq. 7*) let's add another line (*Eq. 8*) crossing the first

- (7) $latex x_2 = -5x_1 + 2$
- (8) $latex x_2 = 3x_1 + 4$

so the plane $latex x_1x_2$ is divided in four different regions:

- C1: above *Eq. 7* and *Eq. 8*
- C2: above *Eq. 7* and below *Eq. 8*
- C3: below *Eq. 7* and above *Eq. 8*
- C4: below *Eq. 7* and *Eq. 8*

Having four regions we need two neurons, one for each line. With two neurons we have two outputs (and four *linearly separable* classes), defined as:

- C1: (output_1, output_2) = (1, 1)
- C2: (output_1, output_2) = (1, -1)
- C3: (output_1, output_2) = (-1, 1)
- C4: (output_1, output_2) = (-1, -1)

Then we need to build a *layer* of perceptrons. Basically what is done is a class containing a list of perceptrons (remember each one is **independent** from the others):

[code language="python"]
class Layer(object):
    """A layer containing two or more perceptrons.
    """
    def __init__(self, input_size, num_perceptrons=2, lrn_rates=[1, 1],
                 activations=[signal, signal]):
        """Not checking if lrn_rates and activations have the length equals to
        num_perceptrons.
        """
        self.perceptrons = [Perceptron(input_size, lrn_rates[i], activations[i])
                            for i in range(num_perceptrons)]

    def fire(self, inputs):
        return [perceptron.fire(inputs) for perceptron in self.perceptrons]

    def training(self, inputs_vector, outputs_vector, max_epochs):
        """outputs_vector is a list containing the same number of elements of
        perceptron. Each element is another list with length equals to inputs_vector's
        length.
        """
        epochs = 0

        for (perceptron, outputs) in zip(self.perceptrons, outputs_vector):
            epochs_per_perceptron = perceptron.training(inputs_vector, outputs,
                                                        max_epochs)
            if not epochs_per_perceptron:
                return epochs_per_perceptron

            epochs = epochs + epochs_per_perceptron

        return epochs
[/code]

It's importante to notice that each *perceptron* receives all inputs, no matter if it's one, two or ten in the layer. Also, as the classes are *linearly separable*, we can train one *perceptron* per time. So, now, our Multiclass Perceptron is composed of two layers: an input layer and an output layer (where the neurons are).

The code to test this new Perceptron of ours (with two neurons) is basically the same:

[code language="python"]
def classify(y, x, a, b):
    if y >= a*x + b:
        return 1 # class C1
    return -1 # class C2

def gen_examples(num_examples, a, b, c, d):
    inputs_vector = []
    outputs_vector = [[], []]

    for _ in range(num_examples):
        x1 = uniform(-10, 10)
        x2 = uniform(-10, 10)

        inputs_vector.append([x1, x2])
        outputs_vector[0].append(classify(x1, x2, a, b))
        outputs_vector[1].append(classify(x1, x2, c, d))

    return (inputs_vector, outputs_vector)

def layer(lrn_rates, num_training, num_test, a, b, c, d):
    layer = Layer(2, lrn_rates=lrn_rates)

    (inputs_vector, outputs_vector) = gen_examples(num_training, a, b, c, d)
    print('#TRAINING (max_epochs = 80)')
    epochs = layer.training(inputs_vector, outputs_vector, 80)
    print(('epochs: %d' % epochs) if epochs else 'training aborted')

    (inputs_vector, outputs_vector) = gen_examples(num_test, a, b, c, d)
    print('#TESTING')
    error_count = load_test(layer, inputs_vector, outputs_vector)
    print('error_count:', error_count)
[/code]

Now, imagine we keep the lines from *Eq. 7* and *Eq. 8*, but again we just have two classes:

- C1: above *Eq. 7* and *Eq. 8*
- C2: otherwise

This means `C2`, `C3` and `C4` are all called just `C2`. There's no way to draw a line separating the new classes `C1` and `C2`, but we could filter the previously shown *layer*'s outputs (two numbers `y1` and `y2`) into just one, using a "*neuron*". This new *neuron*'s filter rule would be something like:

- C1: (`y1`, `y2`) = (1, 1)
- C2: otherwise

Using the output *neuron* with a fixed *bias* in the interval (`1.5`, `-2`] we could define who is `C1` and who is `C2`. But this *filter* would just be used after training the ANN, as it's weights must always be `1`, and throught it's output is impossible to reach `y1` and `y2`. The *activation function* is not **bijective**.

This network described above is a draft of an ANN known as **Multilayer Perceptron**. But before that, as I always do (*bastard*), let me show you the *Adaline* network.

## 4 - Adaline

Adaline comes from **ADAptive LINear Element** (previously **ADAptive LInear NEuron**), an ANN based on the *McCulloch-Pitts perceptron*, but without the *activation function* (or, more precisely, a *linear activation function*). It was developed by **Bernard Widrow** and **Ted Hoff** at **Stanford University** in 1960. Similar to the Perceptron, the Adaline can classify multiple classes, when it's called Madaline (from Multiclass Adaline), a layer of independent Adalines. It's output is given by:

- (9) $latex y = \sum_{j=1}^{n} w_jx_j + b$

As we know, the *perceptron*'s learning rule is given by

- (10) $latex w_j(n + 1) = w_j(n) + \eta(d - y)x_j$
- (11) $latex b(n + 1) = b(n) + \eta(d - y)$

where $latex \eta$ is the *learning rate*, $latex d$ is the desired output and $latex y$ is the actual output. This can be composed into $latex \Delta w_j = \eta(d - y)x_j$ and *Eq. 10* and *Eq. 11* can be rewritten as:

- (12) $latex w_j(n + 1) = w_j(n) + \Delta w_j$
- (13) $latex b(n + 1) = b(n) + \Delta w_0$ (remember $latex w_0 = b$ and $latex x_0 = +1$)

The perceptron's *learning rule*, when using the *threshold function*, is called *Hebb's rule*. In our case we used the *signal function*, but the essence is the same. The rule used in our *adaline* is the *delta rule*, which uses *gradient descent* to adjust the weights. For an online training, the cost is $latex \varepsilon (w) = \frac{1}{2}(d - y)^2$ and $latex \Delta w_j(n) = \eta(-\frac{\partial \varepsilon}{\partial w_j})$, resulting in *Eq. 10* and *Eq. 11*. So, in practice *perceptron* and *adaline* have the same training rule (the *delta rule*), but the weights in the first are adjusted discretly, while in the second, continuously.

As *activation function* to this adaline, I used a linear function limited between `-1` and `1`:

[code language="python"]
def limited_linear(x):
    if x > 1:
        return 1
    elif x < -1:
        return -1
    return x
[/code]

Also, as the convergence takes a long time, I used a maximum MSE as *stopping rule* (*cross-validation*):

[code language="python"]
class Adaline():
    """Online learning Adaline.
    """
    def __init__(self, input_size, lrn_rate=1):
        """'input_size' is the length of the input.
        'lrn_rate' is the learning rate.
        """
        self.weights = [0]*input_size
        self.bias = 0
        self.lrn_rate = lrn_rate

    def fire(self, input_vector):
        summed = sum([i*w for (i,w) in zip(input_vector, self.weights)])
        return limited_linear(summed + self.bias)


    def training(self, training_examples, validation_examples, max_mse):
        validation_size = len(validation_examples)

        while True:
            # training
            for (input_vector, desired) in training_examples:
                output = self.fire(input_vector)
                error = desired - output

                if error != 0:
                    delta = self.lrn_rate*error
                    self.weights = [(weight + delta*x) for (weight, x) in
                                    zip(self.weights, input_vector)]
                    self.bias = self.bias + delta

            # validation
            mse = 0
            for (input_vector, desired) in validation_examples:
                output = self.fire(input_vector)
                error = desired - output
                mse = mse + error*error
            mse = mse/validation_size

            if mse <= max_mse:
                return mse
[/code]

The rest of the code is similar to the previous ones. The whole code is <a href="https://github.com/embatbr/The-Men-Who-Stare-at-Codes/tree/master/posts/neural-networks-in-a-nutshell-3/code" target="_blank">here</a>.

## 5 - Multilayer Perceptron

## References

- <a href="http://www.amazon.com/Neural-Networks-Learning-Machines-Edition/dp/0131471392" target="_blank">Neural Networks and Learning Machines – 3rd Ed – Haykin, Simon – Pearson</a>.
- <a href="http://prolland.free.fr/works/ai/docs/neuro-intro.pdf" target="_blank">An introduction to Neural Networks – 8th Ed. – Kröse, Ben; van der Smagt, Patrick – University of Amsterdam</a>.
- <a href="http://en.wikipedia.org/wiki/Delta_rule" target="_blank">Delta rule - Wikipedia</a>.
- <a href="http://en.wikipedia.org/wiki/Mean_squared_error" target="_blank">Mean Squared Error - Wikipedia</a>.









