### Backpropagation

The error generation shown in *Fig. 9* is self explanatory:

[caption id="attachment_601" align="aligncenter" width="510"]<a href="http://themenwhostareatcodes.files.wordpress.com/2014/03/figure-09.png"><img class=" wp-image-601  " alt="Figure 9 Signal-flow graph of the error generation." src="http://themenwhostareatcodes.files.wordpress.com/2014/03/figure-09.png?w=300" width="510" height="333" /></a> <strong>Figure 9</strong> Signal-flow graph of the error generation.[/caption]

The nodes named $latex y_i(n)$ represents the inputs for neuron *j* (*n* is the example number), weighted summed into variable $latex v_j(n)$

- (18) $latex v_j(n) = \sum_{i=0}^{m}w_{ji}(n)y_i(n)$

where there are *m* inputs ($latex y_i(n) = +1$ and $latex w_{j0}(n) = b_j$) applied. Hence, the neuron's output at iteration *n* is

- (19) $latex y_j(n) = \varphi(v_j(n))$

Similar to the LMS algorithm (*Eq. 12* and *Eq. 13*) we apply a correction

- (20) $latex \Delta w_{ji}(n) = -\eta\frac{\partial\varepsilon(n)}{\partial w_{ji}(n)}$

According to the *chain rule* (remember calculus?), we may express $latex \frac{\partial\varepsilon(n)}{\partial w_{ji}(n)}$ as

- (21) $latex \frac{\partial\varepsilon(n)}{\partial w_{ji}(n)} = \frac{\partial\varepsilon(n)}{\partial e_j(n)}\frac{\partial e_j(n)}{\partial y_j(n)}\frac{\partial y_j(n)}{\partial v_j(n)}\frac{\partial v_j(n)}{\partial w_{ji}(n)}$

The partial derivative of *Eq. 21* represents a *sensitivity factor*, determining the direction of search in weight space for the synaptic weight $latex w_{ji}$. So, let's take the left side of *Eq. 21* piece by piece. Differentiating both sides of *Eq. 16* with respect to $latex e_j(n)$, we get

- (22) $latex \frac{\partial\varepsilon(n)}{\partial e_j(n)} = e_j(n)$

For *Eq. 14*, we get

- (23) $latex \frac{\partial e_j(n)}{\partial y_j(n)} = -1$

Now, for *Eq. 19*, we get

- (24) $latex \frac{\partial y_j(n)}{\partial v_j(n)} = \varphi_j'(v_j(n))$

where the $latex '$ signifies differentiation with respect to the argument. Finally, differentiating *Eq. 18* with respect to $latex w_{ji}$ yields

- (25) $latex \frac{\partial v_j(n)}{\partial w_{ji}(n)} = y_i(n)$

Now, *Eq. 21* became

- (26) $latex \frac{\partial\varepsilon(n)}{\partial w_{ji}(n)} = -e_j(n)\varphi_j'(v_j(n))y_i(n)$

Then, *Eq. 20* turns into

- (27) $latex \Delta w_{ji}(n) = \eta e_j(n)\varphi_j'(v_j(n))y_i(n) = \eta \delta_j(n)y_i(n)$

where $latex \delta_j(n) = e_j(n)\varphi_j'(v_j(n))$ is the *local gradient*. Reading *Eq. 27* we notice that the *delta rule* shown in the previous sections did not present the differentiation of the activation function ($latex \varphi_j'(v_j(n))$).

From these last two equations we note that a key factor in the calculation of $latex \Delta w_{ji}(n)$ is the error signal $latex e_j(n)$ at the output of neuron *j*. We can identify two distinct cases, depending of the location of this node. In **case 1**, neuron *j* is in the output layer. This case is simple and we have an idea of how to proceed. In **case 2**, neuron *j* belongs to a *hidden layer*. Even without being touchable, a hidden neuron shares the responsibility of the error presented in the output of the network. This is known as the *credit-assignment problem* or "who is to blame?". So, let's see in more details how to proceed when neuron *j* fits in **case 2**.